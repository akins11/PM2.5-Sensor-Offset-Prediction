{"title":"Sensor Offset Prediction","markdown":{"yaml":{"title":"Sensor Offset Prediction","jupyter":"python3"},"headingText":"Libraries","containsRefs":false,"markdown":"\n\n\n\n```{python}\n\n\nfrom sklearn.model_selection import KFold, cross_validate, cross_val_score, cross_val_predict  # train_test_split,\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_score, recall_score\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *\n\nimport function as cfun\n\ntrain = pd.read_csv(\"data/train.csv\", parse_dates = ['Datetime'])\n\n```\n\n### Feature Engineering\nFor the training set all missing values will be removed from the data.  \nAdditional variables such as an ordinal air quality index (as integer) for both sensors will be added to the training set.  \nAlso the hour and day of the year will be extracted from the datetime variable after which the datetime variable will be dropped with the Id variable as they are both features with high cardinality.\n\n\n```{python}\nord_train = train.sort_values(by=\"Datetime\")\n\nadd_df = cfun.add_attributes(ord_train, drop_nan_value=True, fill_nan_value=False)\n\ntrain_c = add_df.drop_missing_value()\ntrain_c = add_df.add_air_quality_index()\ntrain_c = add_df.add_period_variables(hour=True, dayofyear=True)\ntrain_c = train_c.drop([\"ID\", \"Datetime\"], axis = 1)\n\ntrain_c.info()\n```\n\n##### Separating the label for the predictors.\n\n```{python}\noutcome = \"Offset_fault\"\nX = train_c.drop(outcome, axis = 1)\ny = train_c[outcome]\n\nfeature_names = list(train_c.drop(outcome, axis = 1).columns)\n```\n\n##### Scale all numeric features\n```{python}\nnum_features = list(X.select_dtypes(\"number\").columns)\nnum_pipeline = Pipeline([\n    (\"std_scaler\", StandardScaler())\n])\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_features)\n])\n\nX = full_pipeline.fit_transform(X)\n\nX\n```\n\n\n\n\n\n\n#### Inital Selected Models\nMultiple models will be used to _see the best that generalize well on the validation set.\n\n```{python}\nlog_reg = LogisticRegression(random_state=11)\ndt_class = DecisionTreeClassifier(random_state=11)\nrf_class = RandomForestClassifier(random_state=11, n_jobs=-1)\nknn_class = KNeighborsClassifier(n_jobs=-1)\n\nmodel = [log_reg, dt_class, rf_class, knn_class]\nmodel_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"K-Neighbors\"]\n```\n\n#### Cross Validation\n\n```{python}\ndef cross_validation(model, x=X, y=y, model_name=\"model\", cv=5):\n    y_pred = cross_val_predict(model, x, y, cv=cv, n_jobs=-1) \n        \n    print(f\"{model_name}\\n{'='*50}\")\n    \n    print(f\"Confusion Matrix ::-\\n{confusion_matrix(y, y_pred)}\")\n    print(50*\"-\",\"\\n\")\n    print(f\"Accuracy :: {accuracy_score(y, y_pred)}\\n\")\n    print(classification_report(y, y_pred))\n```\n\nFor better model performance evaluation the training set will be divided into a smaller training set and a validation set (default will be 5 splits).\n```{python}\nfor mdl, mdl_name in zip(model, model_names):\n    cross_validation(mdl, model_name=mdl_name)\n    print(\"\\n\\n\")\n```\n\nOut of all the inital selected models, The Random Forest model have the best performance when we look at it accuracy score in predicting sensor device signal offsets. The model also looks promising in generalizing well on other data.\n\n\n\n```{python}\ndef eval_gs(gs, output=\"best_estimator\"):\n    if output == \"best_estimator\":\n        return gs.best_estimator_\n    elif output == \"best_param\":\n        return gs.best_params_\n    elif output == \"scores_table\":\n        cv_res = gs.cv_results_\n        \n        f_df = pd.DataFrame(cv_res[\"params\"])\n        f_df[\"mean_test_score\"] = cv_res[\"mean_test_score\"]\n        f_df[\"rank_test_score\"] = cv_res[\"rank_test_score\"]\n        f_df[\"mean_train_score\"] = cv_res[\"mean_train_score\"]\n        return f_df.sort_values(by=\"rank_test_score\", ascending=True)\n    \n    elif output == \"feature_importance\":\n        feature_importances = grid_search.best_estimator_.feature_importances_\n        feat_imp = pd.DataFrame(sorted(zip(feature_names, feature_importances), reverse=True), columns = [\"importance_score\", \"Feature\"])\n        return feat_imp.sort_values(by = \"Feature\", ascending=False)\n    else:\n        raise ValueError(\"`output` variable was given a wrong value.\")\n```\n\n\n#### Hyperparameter Tuning\nUsing multiple random forest parameters to train the model on the data, in oreder to get the best combination of hyperparameter values.\n```{python}\nparam_grid = {\"n_estimators\": [100, 200, 300], \"max_leaf_nodes\": [10, 16], 'max_features':[3, 4]}\n\ngrid_search = GridSearchCV(rf_class, param_grid, cv=4, n_jobs=-1, return_train_score=True)\n\ngrid_search.fit(X, y)\n```\n\n\n##### Best Estimators\n```{python}\neval_gs(grid_search)\n```\n\n\n```{python}\neval_gs(grid_search, \"best_param\")\n```\n\n\n```{python}\neval_gs(grid_search, \"scores_table\")\n```\n\n\n##### Feature Importance\nFinding the relative importance of each feature for making accurate predictions.\n```{python}\nft_imp = eval_gs(grid_search, \"feature_importance\")\nft_imp\n```\n\n\n```{python}\n(\n    ggplot(ft_imp, aes(x=\"reorder(importance_score, Feature)\", y=\"Feature\")) +\n    geom_col(fill=\"#788BFF\") +\n    coord_flip() +\n    labs(x=\"\", y=\"\", title=\"Feature Importance\") +\n    theme_light() +\n    theme(plot_title= element_text(color=\"#8F8F8F\"))\n)\n```\n\n\n### Engineering The Test Set\nAll missing values will be imputed with their respective median value and all other feature transformation done on the train set will be used on the test set.\n\n\n```{python}\ntest = pd.read_csv('data/test.csv', parse_dates = ['Datetime'])\n\nord_test = test.sort_values(by=\"Datetime\").reset_index(drop=True)\n\nadd_df = cfun.add_attributes(ord_test, drop_nan_value=False, fill_nan_value=True)\n\ntest_c = add_df.fill_missing_value(fill_fun = \"median\")\ntest_c = add_df.add_air_quality_index()\ntest_c = add_df.add_period_variables(hour=True, dayofyear=True)\ntest_c = test_c.drop([\"ID\", \"Datetime\"], axis = 1)\n\ntest_c = full_pipeline.transform(test_c)\n```\n\n\n\n```{python}\nfinal_model = grid_search.best_estimator_\n\nfinal_prediction = final_model.predict(test_c)\n```\n\n\n```{python}\nsamplesubmission = pd.read_csv('data/SampleSubmission.csv')\n```\n\n```{python}\naccuracy_score(samplesubmission[\"Offset_fault\"], final_prediction)\n```\n\n\n```{python}\nconfusion_matrix(samplesubmission[\"Offset_fault\"], final_prediction)\n```\n\n\n```{python}\n#| warning: false\n#| message: false\nprint(classification_report(samplesubmission[\"Offset_fault\"], final_prediction))\n```\n\nThe test set seems to have an unusual task of predicting just one class which was the time the PM sensors where considered to have no offset faults. That been said, the model only detect that there were no fault in the sensor signals 79% of the time. Given that there are only 0s i.e non offset sensor signals we have a percision of 100%.\n\n\n#### Saving Fitted Model\n```{python}\n#| eval: false\nimport pickle \n\nwith open(\"pm2.5_sensor_offset.pkl\", \"wb\") as f:\n    pickle.dump(final_model, f)\n```\n\n##### Function to easily make future predictions\n```{python}\n#| eval: false\ndef make_predictions(data_file_path, model_file_path):\n    \"\"\"\n    param: data_file_path : The file path to the new set of records.\n    param: model_file_path: The file path to the pickle serialized file.\n\n    return: pandas serise with predicted values.\n    \"\"\"\n\n    # data transformation\n    from function import add_attributes\n    from pandas import Series\n    ord_rec = test.sort_values(by=\"Datetime\").reset_index(drop=True)\n\n    add_df = add_attributes(ord_rec, drop_nan_value=False, fill_nan_value=True)\n\n    rec_c = add_df.fill_missing_value(fill_fun = \"median\")\n    rec_c = add_df.add_air_quality_index()\n    rec_c = add_df.add_period_variables(hour=True, dayofyear=True)\n    rec_c = rec_c.drop([\"ID\", \"Datetime\"], axis = 1)\n\n    rec_c = full_pipeline.transform(rec_c)\n\n    # Load model\n    with open(model_file_path, \"rb\") as f:\n        model = pickle.load(f)\n    \n    # Generate predictions\n    y_preds = model.predict(rec_c)\n\n    # keep predictions in a pandas series\n    y_preds = Series(y_preds, index=ord_rec, name=\"pm2.5_sensor_offsets\")\n\n    return y_preds\n\n```"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"model_page.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.149","jupyter":"python3","theme":"zephyr","toc-title":"Table Of Content","backgroundcolor":"#FEFEFF","page-layout":"full","title":"Sensor Offset Prediction"},"extensions":{"book":{"multiFile":true}}}}}